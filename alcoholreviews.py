import pandas as pd
import streamlit as st
import re
from collections import Counter

# è®¾ç½®é¡µé¢å®½åº¦å’Œæ ‡é¢˜
st.set_page_config(page_title="é…’ç²¾ç¬”å–ç‚¹æ¸—é€çœ‹æ¿", layout="wide")

# --- 1. æå…¶åŸºç¡€çš„åˆ†è¯å‡½æ•° ---
def get_title_keywords(title):
    # \b\w{3,}\b åŒ¹é…é•¿åº¦å¤§äºç­‰äº3çš„å•è¯æˆ–æ•°å­—ï¼ˆä¿ç•™120, 72ç­‰è§„æ ¼ï¼‰
    words = re.findall(r'\b\w{3,}\b', str(title).lower())
    
    # ä»…ä¿ç•™æœ€åŸºç¡€çš„è¯­æ³•è™šè¯
    stop_words = {'and', 'the', 'with', 'for', 'based', 'from', 'this', 'that', 'these', 'those'}
    
    # æ ‡é¢˜å†…éƒ¨å»é‡ï¼šä¸€ä¸ª ASIN æ ‡é¢˜é‡Œå‡ºç°å¤šæ¬¡åŒæ ·çš„è¯åªè®°ä¸€æ¬¡
    return list(set([w for w in words if w not in stop_words]))

# --- 2. æ ¸å¿ƒåˆ†æé€»è¾‘ ---
def analyze_market_echo(df):
    # åŸºç¡€åˆ—åæ ¡éªŒ
    required_cols = ['ASIN', 'Title', 'Review Content']
    if not all(col in df.columns for col in required_cols):
        st.error(f"æ•°æ®ç¼ºå¤±å…³é”®åˆ—ï¼Œè¯·ç¡®ä¿æ–‡ä»¶åŒ…å«: {required_cols}")
        return pd.DataFrame(), 0, 0

    # é¢„å¤„ç†ç¼ºå¤±å€¼
    df['Title'] = df['Title'].fillna('')
    df['Review Content'] = df['Review Content'].fillna('')
    
    total_asins = df['ASIN'].nunique()
    total_reviews = len(df)

    # --- A. æ ‡é¢˜ç«¯ç»Ÿè®¡ (æŒ‰ ASIN å»é‡) ---
    # æ¯ä¸ª ASIN ä»…å–å…¶ç¬¬ä¸€è¡Œ Title è¿›è¡Œè¯é¢‘åˆ†æ
    asin_level_df = df.groupby('ASIN')['Title'].first().reset_index()
    asin_level_df['kw_list'] = asin_level_df['Title'].apply(get_title_keywords)
    
    all_title_words = []
    for ks in asin_level_df['kw_list']:
        all_title_words.extend(ks)
    
    kw_counts = Counter(all_title_words)
    # å–æ ‡é¢˜ä¸­å‡ºç°é¢‘ç‡æœ€é«˜çš„å‰ 100 ä¸ªå…³é”®è¯
    top_kws = [item[0] for item in kw_counts.most_common(100)]

    # --- B. è¯„è®ºç«¯ç»Ÿè®¡ (æŒ‰ è¯„è®ºè¡Œæ•° ç»Ÿè®¡) ---
# --- æ ¸å¿ƒåˆ†æå¾ªç¯ä¿®æ”¹ ---
    analysis_data = []
    for kw, synonyms in EXTENDED_MAPPING.items():
        # 1. æ‰¾åˆ°æ ‡é¢˜é‡ŒåŒ…å«è¯¥æ ¸å¿ƒè¯çš„ ASIN åˆ—è¡¨
        pattern_title = fr'\b{re.escape(kw)}\b'
        relevant_asins = asin_level_df[asin_level_df['Title'].str.contains(pattern_title, na=False)]['ASIN'].tolist()
        
        title_mentions = len(relevant_asins) # æ ‡é¢˜æ¸—é€æ•°
        title_penetration = (title_mentions / total_asins) * 100
        
        # 2. ä»…é’ˆå¯¹è¿™äº› ASIN çš„è¯„è®ºè¿›è¡Œåˆ†æ
        if title_mentions > 0:
            relevant_reviews_df = df[df['ASIN'].isin(relevant_asins)]
            specific_total_reviews = len(relevant_reviews_df) # è¯¥å–ç‚¹å…³è”çš„æ€»è¯„è®ºåˆ†æ¯
            
            # åŒ¹é…å»¶ä¼¸è¯åº“ï¼ˆè¯­ä¹‰ä¸›ï¼‰
            pattern_review = r'\b(' + '|'.join([re.escape(w) for w in synonyms]) + r')\b'
            review_mentions = relevant_reviews_df['Review Content'].str.contains(pattern_review, na=False).sum()
            
            # å…³é”®ä¿®æ”¹ï¼šé™¤ä»¥è¯¥å–ç‚¹å…³è”çš„è¯„è®ºæ€»æ•°
            review_echo_rate = (review_mentions / specific_total_reviews) * 100
        else:
            review_mentions = 0
            review_echo_rate = 0
            specific_total_reviews = 0

        analysis_data.append({
            "å…³é”®è¯": kw,
            "æ ‡é¢˜æåŠASINæ•°": title_mentions,
            "æ ‡é¢˜æ¸—é€ç‡ (%)": round(title_penetration, 2),
            "å…³è”è¯„è®ºæ€»æ•°": specific_total_reviews, # æ–°å¢è¾…åŠ©åˆ—ï¼Œæ–¹ä¾¿æ’æŸ¥
            "è¯­ä¹‰å‘½ä¸­æ¬¡æ•°": review_mentions,
            "è¯„è®ºå›å£°ç‡ (%)": round(review_echo_rate, 2),
            "å¿ƒæ™ºè½¬åŒ–æ¯”": round(review_echo_rate / (title_penetration if title_penetration > 0 else 1), 2)
        })

    result_df = pd.DataFrame(analysis_data)
    # è®¡ç®—è½¬åŒ–æ•ˆç‡ï¼šå›å£°ç‡ / æ¸—é€ç‡
    result_df['å¿ƒæ™ºè½¬åŒ–æ¯”'] = (result_df['è¯„è®ºå›å£°ç‡ (%)'] / result_df['æ ‡é¢˜æ¸—é€ç‡ (%)']).round(2)
    
    return result_df, total_asins, total_reviews

# --- 3. Streamlit å±•ç¤ºå±‚ ---
st.title("ğŸ¯ å–ç‚¹å›å£°åˆ†æçœ‹æ¿")
st.markdown("""
é€šè¿‡å¯¹æ¯” **å•†å®¶å®£ä¼ ï¼ˆæ ‡é¢˜ï¼‰** ä¸ **ç”¨æˆ·å¤è¿°ï¼ˆè¯„è®ºï¼‰**ï¼Œè¯†åˆ«çœŸå®çš„å¸‚åœºå¿ƒæ™ºã€‚
- **æ ‡é¢˜æ¸—é€ç‡**: å¸‚åœºä¸Šæœ‰å¤šå°‘æ¯”ä¾‹çš„ ASIN åœ¨å–è¿™ä¸ªç‚¹ã€‚
- **è¯„è®ºå›å£°ç‡**: æœ‰å¤šå°‘æ¯”ä¾‹çš„ç”¨æˆ·è¯„è®ºåœ¨åé¦ˆè¿™ä¸ªç‚¹ã€‚
""")

uploaded_file = st.file_uploader("ä¸Šä¼ æ‚¨çš„æ•°æ®æ–‡ä»¶ (Excel æˆ– CSV)", type=['csv', 'xlsx'])

if uploaded_file:
    # æ ¹æ®åç¼€è¯»å–æ•°æ®
    if uploaded_file.name.endswith('.csv'):
        df_input = pd.read_csv(uploaded_file)
    else:
        df_input = pd.read_excel(uploaded_file)
    
    # æ‰§è¡Œåˆ†æ
    res_df, total_a, total_r = analyze_market_echo(df_input)
    
    if not res_df.empty:
        # æŒ‡æ ‡æ€»è§ˆå¡ç‰‡
        m1, m2 = st.columns(2)
        m1.metric("åˆ†æ ASIN æ€»æ•°", total_a)
        m2.metric("åˆ†æè¯„è®ºæ€»æ¡æ•°", total_r)

        st.divider()

        # æ•°æ®è¡¨æ ¼å±•ç¤º
        st.subheader("ğŸ“Š å…³é”®è¯å–ç‚¹è½¬åŒ–æ¸…å•")
        
        # é»˜è®¤æŒ‰è¯„è®ºå›å£°ç‡æ’åº
        res_df = res_df.sort_values("è¯„è®ºå›å£°ç‡ (%)", ascending=False)
        
        # åº”ç”¨èƒŒæ™¯æ¸å˜è‰²ï¼Œå¢å¼ºå¯è¯»æ€§
        styled_df = res_df.style.background_gradient(
            subset=['æ ‡é¢˜æ¸—é€ç‡ (%)', 'è¯„è®ºå›å£°ç‡ (%)', 'å¿ƒæ™ºè½¬åŒ–æ¯”'], 
            cmap='GnBu'
        )
        
        st.dataframe(styled_df, use_container_width=True)
        
        st.info("""
        **æŒ‡æ ‡è§£é‡Šï¼š**
        1. **æ ‡é¢˜æ¸—é€ç‡ (%)**: é…’ç²¾ç¬”å¸‚åœºä¸­ï¼Œæœ‰å¤šå°‘å–å®¶åœ¨æ ‡é¢˜é‡Œä½¿ç”¨äº†è¿™ä¸ªè¯ã€‚
        2. **è¯„è®ºå›å£°ç‡ (%)**: ä¹°äº†é…’ç²¾ç¬”çš„ç”¨æˆ·ï¼Œæœ‰å¤šå°‘äººåœ¨è¯„è®ºé‡Œæåˆ°äº†è¿™ä¸ªè¯ã€‚
        3. **å¿ƒæ™ºè½¬åŒ–æ¯”**: è¯„è®ºå›å£°ç‡ Ã· æ ‡é¢˜æ¸—é€ç‡ã€‚æ•°å€¼è¶Šé«˜ï¼Œä»£è¡¨è¿™ä¸ªè¯å¯¹ç”¨æˆ·çš„å¿ƒæ™ºè§¦è¾¾è¶Šå¼ºã€‚
        """)
else:
    st.warning("ğŸ‘ˆ è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶è¿›è¡Œåˆ†æã€‚")
